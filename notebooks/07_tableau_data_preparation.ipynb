{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c80ee6",
   "metadata": {},
   "source": [
    "# Tableau Data Preparation\n",
    "\n",
    "This notebook transforms the carbon intensity analysis data into formats optimised for Tableau visualisation. The main challenge is converting the wide-format regional data (where each UK region is a separate column) into long format that Tableau can handle effectively for geographic analysis and regional comparisons.\n",
    "\n",
    "## Key Transformations:\n",
    "1. **Regional Data Reshaping**: Convert wide format to long format for proper regional analysis\n",
    "2. **Geographic Categorisation**: Add country and region type classifications\n",
    "3. **Temporal Features**: Extract hour, day, season for time-based analysis\n",
    "4. **Export Optimised Datasets**: Create both full and sample datasets for Tableau performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc17308",
   "metadata": {},
   "source": [
    "#### Stage 1: Environment Setup\n",
    "Import the necessary libraries for data transformation and export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dac3b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "Pandas version: 2.3.1\n",
      "Numpy version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c06dcb5",
   "metadata": {},
   "source": [
    "#### Stage 2: Load the Combined Dataset\n",
    "Load the processed combined carbon intensity data from our previous analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa20cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading combined carbon intensity dataset...\n",
      "Dataset loaded: 121,034 rows × 48 columns\n",
      "Date range: 2018-09-17 23:00:00+00:00 to 2025-08-13 11:30:00+00:00\n",
      "Memory usage: 44.3 MB\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "project_root = Path().absolute().parent\n",
    "data_path = project_root / 'data' / 'processed'\n",
    "output_path = project_root / 'data' / 'processed'\n",
    "\n",
    "# Load the combined dataset\n",
    "print(\"Loading combined carbon intensity dataset...\")\n",
    "combined_df = pd.read_csv(data_path / 'combined_generation_carbon_intensity.csv')\n",
    "\n",
    "# Convert datetime column\n",
    "combined_df['datetime'] = pd.to_datetime(combined_df['datetime'])\n",
    "\n",
    "print(f\"Dataset loaded: {combined_df.shape[0]:,} rows × {combined_df.shape[1]} columns\")\n",
    "print(f\"Date range: {combined_df['datetime'].min()} to {combined_df['datetime'].max()}\")\n",
    "print(f\"Memory usage: {combined_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea92cf1c",
   "metadata": {},
   "source": [
    "#### Stage 3: Regional Data Structure Analysis\n",
    "Examine the current regional data structure and identify the columns that need reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f58e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regional columns in dataset: 14\n",
      "Available regions: ['North Scotland', 'South Scotland', 'North West England', 'North East England', 'Yorkshire', 'North Wales and Merseyside', 'South Wales', 'West Midlands', 'East Midlands', 'East England', 'South West England', 'South England', 'London', 'South East England']\n",
      "\n",
      "Current wide format structure (first few regional columns):\n",
      "                   datetime  CARBON_INTENSITY  North Scotland  South Scotland  \\\n",
      "0 2018-09-17 23:00:00+00:00              83.0            30.0            11.0   \n",
      "1 2018-09-17 23:30:00+00:00              83.0            44.0             7.0   \n",
      "2 2018-09-18 00:00:00+00:00              78.0            44.0             8.0   \n",
      "3 2018-09-18 00:30:00+00:00              77.0            44.0            11.0   \n",
      "4 2018-09-18 01:00:00+00:00              76.0            45.0            13.0   \n",
      "\n",
      "   North West England  \n",
      "0                38.0  \n",
      "1                41.0  \n",
      "2                39.0  \n",
      "3                37.0  \n",
      "4                34.0  \n"
     ]
    }
   ],
   "source": [
    "# Identify regional columns from the data dictionary\n",
    "regional_columns = [\n",
    "    'North Scotland', 'South Scotland', 'North West England', \n",
    "    'North East England', 'Yorkshire', 'North Wales and Merseyside',\n",
    "    'South Wales', 'West Midlands', 'East Midlands', 'East England',\n",
    "    'South West England', 'South England', 'London', 'South East England'\n",
    "]\n",
    "\n",
    "# Verify all regional columns exist in the dataset\n",
    "missing_regions = [col for col in regional_columns if col not in combined_df.columns]\n",
    "available_regions = [col for col in regional_columns if col in combined_df.columns]\n",
    "\n",
    "print(f\"Regional columns in dataset: {len(available_regions)}\")\n",
    "print(f\"Available regions: {available_regions}\")\n",
    "\n",
    "if missing_regions:\n",
    "    print(f\"Missing regional columns: {missing_regions}\")\n",
    "\n",
    "# Show sample of current wide format\n",
    "print(\"\\nCurrent wide format structure (first few regional columns):\")\n",
    "sample_cols = ['datetime', 'CARBON_INTENSITY'] + available_regions[:3]\n",
    "print(combined_df[sample_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aec7570",
   "metadata": {},
   "source": [
    "#### Stage 4: Define Helper Functions\n",
    "Create functions to categorise regions and prepare data for transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07bfb526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_country(region_name):\n",
    "    \"\"\"Classify regions by country (Scotland, Wales, England)\"\"\"\n",
    "    if 'Scotland' in region_name:\n",
    "        return 'Scotland'\n",
    "    elif 'Wales' in region_name:\n",
    "        return 'Wales'\n",
    "    elif 'England' in region_name or region_name == 'London':\n",
    "        return 'England'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "def classify_region_type(region_name):\n",
    "    \"\"\"Classify regions by geographic orientation\"\"\"\n",
    "    if 'North' in region_name:\n",
    "        return 'Northern'\n",
    "    elif 'South' in region_name:\n",
    "        return 'Southern'\n",
    "    elif 'East' in region_name:\n",
    "        return 'Eastern'\n",
    "    elif 'West' in region_name or 'Wales' in region_name:\n",
    "        return 'Western'\n",
    "    elif region_name == 'London':\n",
    "        return 'Metropolitan'\n",
    "    elif region_name == 'Yorkshire':\n",
    "        return 'Central'\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b858a3",
   "metadata": {},
   "source": [
    "#### Stage 5: Transform Regional Data to Long Format\n",
    "Convert the wide format regional data into long format suitable for Tableau analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e95a7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting regional data transformation...\n",
      "Converting 14 regional columns to long format...\n",
      "Preserving 34 non-regional columns as identifiers\n",
      "\n",
      "Transformation complete!\n",
      "Original dataset: 121,034 rows × 48 columns\n",
      "Long format dataset: 1,694,476 rows × 40 columns\n",
      "Multiplication factor: 14x (one row per region per timestamp)\n"
     ]
    }
   ],
   "source": [
    "def create_tableau_regional_dataset(df, regional_cols):\n",
    "    \"\"\"Transform wide regional format to long format for Tableau\"\"\"\n",
    "    \n",
    "    # Identify non-regional columns (these will be ID variables in melt)\n",
    "    non_regional_cols = [col for col in df.columns if col not in regional_cols]\n",
    "    \n",
    "    print(f\"Converting {len(regional_cols)} regional columns to long format...\")\n",
    "    print(f\"Preserving {len(non_regional_cols)} non-regional columns as identifiers\")\n",
    "    \n",
    "    # Melt the dataframe to convert wide to long format\n",
    "    df_long = df.melt(\n",
    "        id_vars=non_regional_cols,\n",
    "        value_vars=regional_cols,\n",
    "        var_name='Region',\n",
    "        value_name='Regional_Carbon_Intensity'\n",
    "    )\n",
    "    \n",
    "    # Add geographic classifications\n",
    "    df_long['Country'] = df_long['Region'].apply(classify_country)\n",
    "    df_long['Region_Type'] = df_long['Region'].apply(classify_region_type)\n",
    "    \n",
    "    # Calculate regional deviation from national average\n",
    "    df_long['Regional_Deviation'] = df_long['Regional_Carbon_Intensity'] - df_long['CARBON_INTENSITY']\n",
    "    df_long['Regional_Deviation_Percent'] = (df_long['Regional_Deviation'] / df_long['CARBON_INTENSITY']) * 100\n",
    "    \n",
    "    return df_long\n",
    "\n",
    "# Apply the transformation\n",
    "print(\"Starting regional data transformation...\")\n",
    "tableau_regional_data = create_tableau_regional_dataset(combined_df, available_regions)\n",
    "\n",
    "print(f\"\\nTransformation complete!\")\n",
    "print(f\"Original dataset: {combined_df.shape[0]:,} rows × {combined_df.shape[1]} columns\")\n",
    "print(f\"Long format dataset: {tableau_regional_data.shape[0]:,} rows × {tableau_regional_data.shape[1]} columns\")\n",
    "print(f\"Multiplication factor: {tableau_regional_data.shape[0] // combined_df.shape[0]}x (one row per region per timestamp)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a695e",
   "metadata": {},
   "source": [
    "#### Stage 6: Export Datasets for Tableau\n",
    "Save the transformed datasets in formats optimised for Tableau import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f505a5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting datasets for Tableau...\n",
      "Output directory: /Users/julianelliott/Documents/GitHub/grid-carbon-analysis/data/processed/tableau\n",
      "Full dataset exported: carbon_intensity_regional_long_format.csv (443.1 MB)\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "tableau_output_path = output_path / 'tableau'\n",
    "tableau_output_path.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Exporting datasets for Tableau...\")\n",
    "print(f\"Output directory: {tableau_output_path}\")\n",
    "\n",
    "# Export full regional dataset\n",
    "full_output_file = tableau_output_path / 'carbon_intensity_regional_long_format.csv'\n",
    "tableau_regional_data.to_csv(full_output_file, index=False)\n",
    "file_size_mb = full_output_file.stat().st_size / 1024**2\n",
    "print(f\"Full dataset exported: {full_output_file.name} ({file_size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a5aa75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix datetime formatting for Tableau compatibility\n",
    "\n",
    "# Convert datetime to string format that Tableau can properly parse\n",
    "tableau_regional_data_fixed = tableau_regional_data.copy()\n",
    "\n",
    "# Format datetime as ISO string without timezone (Tableau prefers this)\n",
    "tableau_regional_data_fixed['datetime'] = tableau_regional_data_fixed['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Create separate date and time columns for easier Tableau analysis\n",
    "tableau_regional_data_fixed['Date_Only'] = pd.to_datetime(tableau_regional_data['datetime']).dt.date\n",
    "tableau_regional_data_fixed['Time_Only'] = pd.to_datetime(tableau_regional_data['datetime']).dt.strftime('%H:%M:%S')\n",
    "\n",
    "# Ensure all numeric columns are properly typed\n",
    "numeric_columns = [\n",
    "    'CARBON_INTENSITY', 'Regional_Carbon_Intensity', 'Regional_Deviation', 'Regional_Deviation_Percent',\n",
    "    'GAS', 'COAL', 'NUCLEAR', 'WIND', 'HYDRO', 'IMPORTS', 'BIOMASS', 'SOLAR',\n",
    "    'GAS_perc', 'COAL_perc', 'NUCLEAR_perc', 'WIND_perc', 'HYDRO_perc', 'RENEWABLE_perc', 'FOSSIL_perc'\n",
    "]\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in tableau_regional_data_fixed.columns:\n",
    "        tableau_regional_data_fixed[col] = pd.to_numeric(tableau_regional_data_fixed[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "191c47c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-parse datetime for temporal feature extraction\n",
    "temp_datetime = pd.to_datetime(tableau_regional_data_fixed['datetime'])\n",
    "\n",
    "tableau_regional_data_fixed['Hour'] = temp_datetime.dt.hour\n",
    "tableau_regional_data_fixed['Day_of_Week'] = temp_datetime.dt.day_name()\n",
    "tableau_regional_data_fixed['Season'] = temp_datetime.dt.month.map({\n",
    "    12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "    3: 'Spring', 4: 'Spring', 5: 'Spring', \n",
    "    6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "    9: 'Autumn', 10: 'Autumn', 11: 'Autumn'\n",
    "})\n",
    "tableau_regional_data_fixed['Time_Period'] = 'Standard'\n",
    "tableau_regional_data_fixed.loc[tableau_regional_data_fixed['Hour'].between(19, 21), 'Time_Period'] = 'Peak'\n",
    "tableau_regional_data_fixed.loc[tableau_regional_data_fixed['Hour'].between(2, 4), 'Time_Period'] = 'Off-Peak'\n",
    "\n",
    "# 1. Create a smaller sample dataset\n",
    "sample_per_region = 10000  # For initial Tableau development\n",
    "\n",
    "# Sample evenly across time periods for each region\n",
    "tableau_sample = []\n",
    "for region in available_regions:\n",
    "    region_data = tableau_regional_data_fixed[tableau_regional_data_fixed['Region'] == region]\n",
    "    if len(region_data) > sample_per_region:\n",
    "        # Sample evenly across the time period\n",
    "        sample_indices = np.linspace(0, len(region_data)-1, sample_per_region, dtype=int)\n",
    "        region_sample = region_data.iloc[sample_indices]\n",
    "    else:\n",
    "        region_sample = region_data\n",
    "    tableau_sample.append(region_sample)\n",
    "\n",
    "tableau_sample_df = pd.concat(tableau_sample, ignore_index=True)\n",
    "\n",
    "# 2. Export the sample dataset\n",
    "sample_file = tableau_output_path / 'carbon_intensity_tableau_sample.csv'\n",
    "tableau_sample_df.to_csv(sample_file, index=False)\n",
    "sample_size_mb = sample_file.stat().st_size / 1024**2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
